<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title></title>
  
  <subtitle>Something is Nothing</subtitle>
  <link href="http://amadeus-z.github.io/atom.xml" rel="self"/>
  
  <link href="http://amadeus-z.github.io/"/>
  <updated>2022-12-28T07:53:58.382Z</updated>
  <id>http://amadeus-z.github.io/</id>
  
  <author>
    <name>Amadeus</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Chapter_6</title>
    <link href="http://amadeus-z.github.io/2022/12/28/Chapter-6/"/>
    <id>http://amadeus-z.github.io/2022/12/28/Chapter-6/</id>
    <published>2022-12-28T07:52:45.000Z</published>
    <updated>2022-12-28T07:53:58.382Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Initial-Value-Problems"><a href="#Initial-Value-Problems" class="headerlink" title="Initial Value Problems"></a>Initial Value Problems</h2><h3 id="Euler’s-Method"><a href="#Euler’s-Method" class="headerlink" title="Euler’s Method"></a>Euler’s Method</h3><h4 id="initial-value-problem"><a href="#initial-value-problem" class="headerlink" title="initial value problem"></a>initial value problem</h4><p>$y’=f(t,y),y(a)=y_{0},t\in [a,b]$ , $f$ represents the slope of the solution.</p><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>divide the interval to $n+1$ points: $t_{0},\dots,t_{n}$ , $h$ is the step size.<br>$w_{0}=y_{0}$<br>$w_{i+1}=w_{i}+hf(t_{i},w_{i})$ </p><h4 id="Existence-of-solutions"><a href="#Existence-of-solutions" class="headerlink" title="Existence of solutions"></a>Existence of solutions</h4><p>Assume that $f(t,y)$ is Lipschitz continuous in the variable $y$ on the set $[a,b]\times [\alpha,\beta]$ and $\alpha &lt; y_{a} &lt;\beta$. Then there exists $c$ between $a$ and $b$ such that the initial value problem has exactly one solution $y(t)$. Moreover, if $f$ is Lipschitz on $[a,b]\times (-\infty,\infty)$, then there exists exactly one solution on $[a,b]$.</p><p><strong>(Banach Fixed Point Theorem For ODE)</strong></p><h4 id="Stability-Error-Amplification"><a href="#Stability-Error-Amplification" class="headerlink" title="Stability (Error Amplification)"></a>Stability (Error Amplification)</h4><p>Assume that $f(t,y)$ is Lipschitz in the variable $y$ on the set $S=[a,b]\times [\alpha,\beta]$.<br>If $Y(t)$ and $Z(t)$ are solutions in $S$ of the differential equation $y’=f(t,y)$ with initial conditions $Y(a)$ and $Z(a)$ respectively, then</p><script type="math/tex; mode=display">|Y(t)-Z(t)|\le e^{L(t-a)}|Y(a)-Z(a)|</script><p>$L$ called the <strong>Lipschitz constant</strong></p><h4 id="First-order-linear-equations"><a href="#First-order-linear-equations" class="headerlink" title="First-order linear equations"></a>First-order linear equations</h4><p>The IVP who are linear in the $y$ variable<br>$y’=g(t)y+h(t)\Rightarrow (y’-g(t)y)e^{-\int g(t)dt}=e^{-\int g(t)dt} h(t)\Rightarrow y(t)=e^{\int g(t)dt}\int e^{-\int g(t)dt}h(t)dt$<br>using $L=\max_{[a,b]}g(t)$ as the Lipschitz constant to prove the unique solution </p><h2 id="Analysis-of-IVP-solvers"><a href="#Analysis-of-IVP-solvers" class="headerlink" title="Analysis of IVP solvers"></a>Analysis of IVP solvers</h2><h3 id="Global-and-Local-truncation-error"><a href="#Global-and-Local-truncation-error" class="headerlink" title="Global and Local truncation error"></a>Global and Local truncation error</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p><strong>global truncation error</strong> $g_{i}=|w_{i}-y_{i}|$ : the difference between approximation and the correct solution.<br><strong>Local truncation error</strong> $e_{i+1}=|w_{i+1}-z(t_{i+1})|$ : the difference between value of the solver on that interval and the correct solution of the “one-step IVP”：$y’=f(t,y);y(t_{i})=w_{i};t\in[t_{i},t_{i+1}]$ </p><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><p>Assume that $f(t,y)$ has a Lipschitz constant $L$ for the variable $y$ and that the value $y_{i}$ of the solution of the IVP at $t_{i}$ is approximated by $w_{i}$ from a one-step ODE solver with local truncation error $e_{i}\le Ch^{k+1}$ , for some constant $C$ and $k\ge 0$ .<br>Then for each $a&lt;t_{i}&lt;b$, the solver has global truncation error:</p><script type="math/tex; mode=display">g_{i}=|w_{i}-y_{i}|\le \frac{Ch^{k}}{L}(e^{L(t_{i}-a)}-1)</script><h4 id="Euler’s-Method-Convergence"><a href="#Euler’s-Method-Convergence" class="headerlink" title="Euler’s Method Convergence"></a>Euler’s Method Convergence</h4><p>Assume that $f(t,y)$ has a Lipschitz constant $L$ for the variable $y$ and that the solution $y_{i}$ of the IVP at $t_{i}$ is approximated by $w_{i}$, using Euler’s Method, Let $M$ be an upper bound for $|y’’(t)|$ on $[a,b]$, Then</p><script type="math/tex; mode=display">|w_{i}-y_{i}|\le \frac{Mh}{2L}(e^{L(t_{i}-a)}-1)</script><h3 id="The-Explicit-Trapezoid-Method"><a href="#The-Explicit-Trapezoid-Method" class="headerlink" title="The Explicit Trapezoid Method"></a>The Explicit Trapezoid Method</h3><p>A small adjustment in Euler’s Method to make an improvement in accuracy</p><h4 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h4><p>$w_{0}=y+0$<br>$w_{i+1}=w_{i}+ \frac{h}{2}(f(t_{i},w_{i})+f(t_{i}+h,w_{i}+hf(t_{i},w_{i})))$</p><h3 id="Taylor-Methods"><a href="#Taylor-Methods" class="headerlink" title="Taylor Methods"></a>Taylor Methods</h3><p>For each positive integer $k$, there is a Taylor Method of order $k$.</p><h4 id="Definition-3"><a href="#Definition-3" class="headerlink" title="Definition"></a>Definition</h4><p>$w_{0}=y_{0}$<br>$w_{i+1}=w_{i}+hf(t_{i},w_{i})+\frac{h^{2}}{2}f’(t_{i},w_{i})+\cdots+ \frac{h^{k}}{k!}f^{(k-1)}(t_{i},w_{i})$<br>The prime notation refers to the total derivative of $f(t,y(t))$ with respect to $t$.</p><h2 id="Systems-of-Ordinary-Differential-Equations"><a href="#Systems-of-Ordinary-Differential-Equations" class="headerlink" title="Systems of Ordinary Differential Equations"></a>Systems of Ordinary Differential Equations</h2><h3 id="Higher-order-equations"><a href="#Higher-order-equations" class="headerlink" title="Higher order equations"></a>Higher order equations</h3><p>Let $y^{(n)}=f(t,y,y’,\dots,y^{(n-1)})$ , define new variables $y_{i}=y^{(i-1)}$ , then $y^{(n)}= y_{n}’=f(t,y_{1},y_{2},\dots,y_{n})$ </p><h3 id="The-Pendulum"><a href="#The-Pendulum" class="headerlink" title="The Pendulum"></a>The Pendulum</h3><p>The differential equation governing the frictionless pendulum is:</p><script type="math/tex; mode=display">mly''=F=-mg\sin y</script><p>Setting $y_{1}=y,y_{2}=y’$, then the second-order equation is converted to a first-order system:<br>$y_{1}’=y_{2}$<br>$y_{2}’=- \frac{g}{l}\sin y_{1}$</p><h3 id="Orbital-mechanics"><a href="#Orbital-mechanics" class="headerlink" title="Orbital mechanics"></a>Orbital mechanics</h3><p>The motion of an orbiting satellite.<br>Place the large mass at the origin, denote the position of the satellite by $(x,y)$. A force on the satellite is in the direction of the large mass, the unit vector:</p><script type="math/tex; mode=display">(-\frac{x}{\sqrt{x^{2}+y^{2}}},-\frac{y}{\sqrt{x^{2}+y^{2}}})</script><script type="math/tex; mode=display">(F_{x},F_{y})=(\frac{gm_{1}m_{2}}{x^{2}+y^{2}}\frac{-x}{\sqrt{x^{2}+y^{2}}},\frac{gm_{1}m_{2}}{x^{2}+y^{2}}\frac{-y}{\sqrt{x^{2}+y^{2}}})</script><p>$\Rightarrow \ m_{1}x’’=-\frac{gm_{1}m_{2}x}{(x^{2}+y^{2})^{\frac{3}{2}}}, \ m_{1}y’’=-\frac{gm_{1}m_{2}y}{(x^{2}+y^{2})^{\frac{3}{2}}}$ , introducing the variables $v_{x}=x’$ and $v_{y}=y’$ allows the two second-order equations to be reduced to a system of four-order equations:<br>$x’=v_{x},v_{x}’=-\frac{gm_{2}x}{(x^{2}+y^{2})^{3/2}}$<br>$y’=v_{y},v_{y}’=-\frac{gm_{2}y}{(x^{2}+y^{2})^{3/2}}$</p><h2 id="Runge-Kutta-Methods-and-Applications"><a href="#Runge-Kutta-Methods-and-Applications" class="headerlink" title="Runge-Kutta Methods and Applications"></a>Runge-Kutta Methods and Applications</h2><h3 id="The-Runge-Kutta-Family"><a href="#The-Runge-Kutta-Family" class="headerlink" title="The Runge-Kutta Family"></a>The Runge-Kutta Family</h3><p>The Runge-Kutta Methods are a family of ODE solvers that include the Euler and Trapezoid Methods and more sophisticated methods of higher order.</p><h4 id="Midpoint-Method"><a href="#Midpoint-Method" class="headerlink" title="Midpoint Method"></a>Midpoint Method</h4><p>second-order methods of R-K type.<br>$w_{0}=y_{0}$<br>$w_{i+1}=w_{i}+hf(t_{i}+ \frac{h}{2},w_{i}+ \frac{h}{2}f(t_{i},w_{i}))$</p><h4 id="Runge-Kutta-Method-of-order-4-RK4"><a href="#Runge-Kutta-Method-of-order-4-RK4" class="headerlink" title="Runge-Kutta Method of order 4 (RK4)"></a>Runge-Kutta Method of order 4 (RK4)</h4><script type="math/tex; mode=display">w_{i+1}=w_{i}+ \frac{h}{6}(s_{1}+2s_{2}+2s_{3}+s_{4})</script><p>where</p><ol><li>$s_{1}=f(t_{i},w_{i})$</li><li>$s_{2}=f(t_{i}+ \frac{h}{2},w_{i}+ \frac{h}{2}s_{1})$</li><li>$s_{3}=f(t_{i}+ \frac{h}{2},w_{i}+ \frac{h}{2}s_{2})$</li><li>$s_{4}=f(t_{i}+h,w_{i}+hs_{3})$</li></ol><h2 id="Variable-Step-Size-Methods"><a href="#Variable-Step-Size-Methods" class="headerlink" title="Variable Step-Size Methods"></a>Variable Step-Size Methods</h2><p>To track the slow change and fast change, the step size $h$ can be variable. The key idea is to monitor the error produced by the current step.</p><ol><li>set an error tolerance that must be met by the current step.</li><li>cut the step size if the error tolerance is exceeded</li><li>accept the step and choose a size for the next step if meet the tolerance</li></ol><h3 id="Embedded-Runge-Kutta-pairs"><a href="#Embedded-Runge-Kutta-pairs" class="headerlink" title="Embedded Runge-Kutta pairs"></a>Embedded Runge-Kutta pairs</h3><p>$to \ be \ continued$</p><h3 id="Order-4-5-methods"><a href="#Order-4-5-methods" class="headerlink" title="Order 4/5 methods"></a>Order 4/5 methods</h3><p>$to \ be \ continued$</p><h2 id="Implicit-Methods-and-Stiff-Equations"><a href="#Implicit-Methods-and-Stiff-Equations" class="headerlink" title="Implicit Methods and Stiff Equations"></a>Implicit Methods and Stiff Equations</h2><p>The method doesn’t directly give a formula for the new approximation $w_{i+1}$.</p><h3 id="Backward-Euler-Method"><a href="#Backward-Euler-Method" class="headerlink" title="Backward Euler Method"></a>Backward Euler Method</h3><p>$w_{0}=y_{0}$<br>$w_{i+1}=w_{i}+hf(t_{i+1},w_{i+1})$</p><h2 id="Multistep-Methods"><a href="#Multistep-Methods" class="headerlink" title="Multistep Methods"></a>Multistep Methods</h2><p>Using the knowledge of more than one of the previous $w_{i}$ to help produce the next step. This leads to ODE solvers have order as high as the one-step methods, but much of the necessary computation will be replaced.</p><h3 id="Adams-Bashforth-Two-Step-Method"><a href="#Adams-Bashforth-Two-Step-Method" class="headerlink" title="Adams-Bashforth Two-Step Method"></a>Adams-Bashforth Two-Step Method</h3><p>$w_{i+1}=w_{i}+h[ \frac{3}{2}f(t_{i},w_{i})- \frac{1}{2}f(t_{i-1},w_{i-1})]$</p><h3 id="Implicit-multistep-methods"><a href="#Implicit-multistep-methods" class="headerlink" title="Implicit multistep methods"></a>Implicit multistep methods</h3><h4 id="Implicit-Trapezoid-Method-Second-Order"><a href="#Implicit-Trapezoid-Method-Second-Order" class="headerlink" title="Implicit Trapezoid Method (Second Order)"></a>Implicit Trapezoid Method (Second Order)</h4><p>$w_{i+1}=w_{i}+ \frac{h}{2}[f_{i+1}+f_{i}]$</p><h4 id="Adams-Moulton-Two-Step-Method-third-order"><a href="#Adams-Moulton-Two-Step-Method-third-order" class="headerlink" title="Adams-Moulton Two-Step Method (third order)"></a>Adams-Moulton Two-Step Method (third order)</h4><p>$w_{i+1}=w_{i}+ \frac{h}{12}[5f_{i+1}+8f_{i}-f_{i-1}]$</p><h4 id="Milne-Slimpson-Method"><a href="#Milne-Slimpson-Method" class="headerlink" title="Milne-Slimpson Method"></a>Milne-Slimpson Method</h4><p>$w_{i+1}=w_{i-1}+ \frac{h}{3}[f_{i+1}+4f_{i}+f_{i-1}]$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Initial-Value-Problems&quot;&gt;&lt;a href=&quot;#Initial-Value-Problems&quot; class=&quot;headerlink&quot; title=&quot;Initial Value Problems&quot;&gt;&lt;/a&gt;Initial Value Proble</summary>
      
    
    
    
    
    <category term="Numerical" scheme="http://amadeus-z.github.io/tags/Numerical/"/>
    
  </entry>
  
  <entry>
    <title>Chapter_5</title>
    <link href="http://amadeus-z.github.io/2022/12/23/Chapter-5/"/>
    <id>http://amadeus-z.github.io/2022/12/23/Chapter-5/</id>
    <published>2022-12-23T13:25:33.000Z</published>
    <updated>2022-12-28T07:54:13.091Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Numerical-Differentiation"><a href="#Numerical-Differentiation" class="headerlink" title="Numerical Differentiation"></a>Numerical Differentiation</h2><h3 id="Finite-difference-formulas"><a href="#Finite-difference-formulas" class="headerlink" title="Finite difference formulas"></a>Finite difference formulas</h3><h4 id="Generalized-Intermediate-Value-Theorem"><a href="#Generalized-Intermediate-Value-Theorem" class="headerlink" title="Generalized Intermediate Value Theorem"></a>Generalized Intermediate Value Theorem</h4><p>Let $f$ be a  continuous function on the interval $[a,b]$. Let $x_{1},\dots,x_{n}$ be points in $[a,b]$, and $a_{1},\dots,a_{n}&gt;0$. Then there exists a number $c$ between $a$ and $b$ such that</p><script type="math/tex; mode=display">(a_{1}+\cdots+a_{n})f(c)=a_{1}f(x_{1})+\cdots+a_{n}f(x_{n})</script><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p><strong>Example:</strong> $f$ is three times continuously differentiable.<br>By Taylor’s Theorem:</p><ol><li>$f(x+h)=f(x)+hf’(x)+\frac{h^{2}}{2}f’’(x)+\frac{h^{3}}{6}f’’’(c_{1})$</li><li>$f(x-h)=f(x)-hf’(x)+\frac{h^{2}}{2}f’’(x)-\frac{h^{3}}{6}f’’’(c_{2})$<br>$\Rightarrow f’(x)=\frac{f(x+h)-f(x-h)}{2h}-\frac{h^{2}}{12}f’’’(c_{1})-\frac{h^{2}}{12}f’’’(c_{2})=\frac{f(x+h)-f(x-h)}{2h}-\frac{h^{2}}{6}f’’’(c)$</li></ol><h4 id="Richardson-Extrapolation"><a href="#Richardson-Extrapolation" class="headerlink" title="(Richardson) Extrapolation"></a>(Richardson) Extrapolation</h4><p>Assume $Q\approx F(h)+Kh^{n}=F(\frac{h}{2})+K(\frac{h}{2})^{n}\Rightarrow Q-F(\frac{h}{2})\approx \frac{1}{2^{n}}(Q-F(h))$<br>$\Rightarrow Q\approx \frac{2^{n}F(\frac{h}{2})-F(h)}{2^{n}-1}$</p><p>It gives a higher-order approximation of $Q$ than $F(h)$.</p><h3 id="Symbolic-differentiation-and-integration"><a href="#Symbolic-differentiation-and-integration" class="headerlink" title="Symbolic differentiation and integration"></a>Symbolic differentiation and integration</h3><p>commands from <strong>Matlab Symbolic Toolbox</strong>:</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">syms x;</span><br><span class="line">f=<span class="built_in">sin</span>(<span class="number">3</span>*x);</span><br><span class="line">f1=diff(f); <span class="comment">% first derivative</span></span><br><span class="line">f3=diff(f,<span class="number">3</span>); <span class="comment">% third derivative</span></span><br><span class="line">int(f); <span class="comment">% integration of f</span></span><br></pre></td></tr></table></figure><h2 id="Newton-Cotes-Formulas-For-Numerical-Integration"><a href="#Newton-Cotes-Formulas-For-Numerical-Integration" class="headerlink" title="Newton-Cotes Formulas For Numerical Integration"></a>Newton-Cotes Formulas For Numerical Integration</h2><h3 id="Trapezoid-Rule"><a href="#Trapezoid-Rule" class="headerlink" title="Trapezoid Rule"></a>Trapezoid Rule</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p>Using Lagrange formulation to interpolate the function</p><script type="math/tex; mode=display">f(x)=y_{0}\frac{x-x_{1}}{x_{0}-x_{1}}+y_{1}\frac{x-x_{0}}{x_{1}-x_{0}}+\frac{(x-x_{0})(x-x_{1})}{2!}f''(c_{x})=P(x)+E(x)</script><p>Denote $h=x_{1}-x_{0}$, $\Rightarrow \ \int^{x_{1}}_{x_{0}}f(x)dx=\frac{h}{2}(y_{0}+y_{1})-\frac{h^{3}}{12}f’’(c)$</p><h3 id="Simpson’s-Rule"><a href="#Simpson’s-Rule" class="headerlink" title="Simpson’s Rule"></a>Simpson’s Rule</h3><h4 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h4><p>Using three points to interpolate the function</p><script type="math/tex; mode=display">f(x)=y_{0}\frac{(x-x_{1})(x-x_{2})}{(x_{0}-x_{1})(x_{0}-x_{2})}+y_{1}\frac{(x-x_{0})(x-x_{2})}{(x_{1}-x_{0})(x_{1}-x_{2})}+\frac{(x-x_{0})(x-x_{1})(x-x_{2})}{3!}f'''(c_{x})=P(x)+E(x)</script><p>Denote $h=x_{2}-x_{1}=x_{1}-x_{0}\Rightarrow \ \int^{x_{2}}_{x_{0}}f(x)dx=\frac{h}{3}(y_{0}+4y_{1}+y_{2})-\frac{h^{5}}{90}f^{(iv)}(c)$</p><h4 id="Simpson’s-3-8-Rule"><a href="#Simpson’s-3-8-Rule" class="headerlink" title="Simpson’s 3/8 Rule"></a>Simpson’s 3/8 Rule</h4><p>$\int_{x_{0}}^{x_{3}}f(x)dx\approx \frac{3h}{8}(y_{0}+3y_{1}+3y_{2}+y_{3})$ </p><h3 id="Composite-Newton-Cotes-Formulas"><a href="#Composite-Newton-Cotes-Formulas" class="headerlink" title="Composite Newton-Cotes Formulas"></a>Composite Newton-Cotes Formulas</h3><p>Trapezoid and Simpson’s rule are limited on a single interval, we can evaluate an intergral by dividing the interval up into several subintervals and apply the rules.</p><h4 id="Composite-Trapezoid-Rule"><a href="#Composite-Trapezoid-Rule" class="headerlink" title="Composite Trapezoid Rule"></a>Composite Trapezoid Rule</h4><script type="math/tex; mode=display">\int^{b}_{a}f(x)dx=\frac{h}{2}(y_{0}+y_{m}+2\sum\limits_{i=1}^{m-1}y_{i})-\frac{(b-a)h^{2}}{12}f''(c)</script><p>where $h=\frac{b-a}{m}$ and $c$ is between $a$ and $b$.</p><h4 id="Composite-Simpson’s-Rule"><a href="#Composite-Simpson’s-Rule" class="headerlink" title="Composite Simpson’s Rule"></a>Composite Simpson’s Rule</h4><script type="math/tex; mode=display">\int^{b}_{a}f(x)dx=\frac{h}{3}(y_{0}+y_{2m}+4\sum\limits_{i=1}^{m-1}y_{2i-1}+2\sum\limits_{i=1}^{m-1}y_{2i})-\frac{(b-a)h^{4}}{180}f^{(iv)}(c)</script><p>where $h=\frac{b-a}{2m}$ and $c$ is between $a$ and $b$.</p><h3 id="Open-Newton-Cotes-Method"><a href="#Open-Newton-Cotes-Method" class="headerlink" title="Open Newton-Cotes Method"></a>Open Newton-Cotes Method</h3><p>Some integrands don’t need to use values from endpoints.</p><h4 id="Midpoint-Rule"><a href="#Midpoint-Rule" class="headerlink" title="Midpoint Rule"></a>Midpoint Rule</h4><p>Let $h=x_{1}-x_{0}, \ w=x_{0}+  \frac{h}{2}$ is the midpoint, and $c$ is between $x_{0}$ and $x_{1}$<br>By Taylor’s theorem: $f(x)=f(w)+(x-w)f’(w)+ \frac{1}{2}(x-w)^{2}f’’(c_{x})$ </p><script type="math/tex; mode=display">\Rightarrow \int^{x_1}_{x_{0}}f(x)dx=hf(w)+ \frac{h^{3}}{24}f''(c)</script><h4 id="Composite-Midpoint-Rule"><a href="#Composite-Midpoint-Rule" class="headerlink" title="Composite Midpoint Rule"></a>Composite Midpoint Rule</h4><script type="math/tex; mode=display">\int^{b}_{a}f(x)dx=h\sum\limits_{i=1}^{m}f(w_{i})+ \frac{(b-a)h^{2}}{24}f''(c)</script><p>where $h=(b-a)/m$ and $c$ is between $a$ and $b$. The $w_{i}$ are the midpoints of the $m$ equal subintervals of $[a,b]$</p><h2 id="Romberg-Integration"><a href="#Romberg-Integration" class="headerlink" title="Romberg Integration"></a>Romberg Integration</h2><p>A result of applying extrapolation to the coposite Trapezoid Rule. Adding data until the required accuracy is attained.</p><h3 id="Richardson-Extrapolation-1"><a href="#Richardson-Extrapolation-1" class="headerlink" title="Richardson Extrapolation"></a>Richardson Extrapolation</h3><p><a href="https://en.wikipedia.org/wiki/Richardson_extrapolation">wiki</a></p><h3 id="Romberg-Integration-1"><a href="#Romberg-Integration-1" class="headerlink" title="Romberg Integration"></a>Romberg Integration</h3><p>By applying Richardson Extrapolation repeatedly on the Trapezoid Rule.</p><p>Since $\int^{b}_{a}f(x)dx=\frac{h}{2}(y_{0}+y_{m}+2\sum\limits_{i=1}^{m-1}y_{i})+c_{2}h^{2}+c_{4}h^{4}+\cdots=A(h)+c_{2}h^{2}+c_{4}h^{4}+\cdots$<br>Let $h_{j}= \frac{1}{2^{j-1}}(b-a), \ R_{11}=\frac{h_{1}}{2}(f(a)+f(b)),R_{21}=\frac{h_{2}}{2}(f(a)+f(b)+2f(\frac{a+b}{2}))=\frac{1}{2}R_{11}+h_{2}f(\frac{a+b}{2})$<br>$\Rightarrow R_{j1}=\frac{1}{2}R_{j-1,1}+h_{j}\sum\limits_{i=1}^{2^{j-2}}f(a+(2i-1)h_{j})$<br>$\Rightarrow R_{jk}=\frac{4^{k-1}R_{j,k-1}-R_{j-1,k-1}}{4^{k-1}-1}$</p><h2 id="Adaptive-Quadrature"><a href="#Adaptive-Quadrature" class="headerlink" title="Adaptive Quadrature"></a>Adaptive Quadrature</h2><p>A criterion to decide what step is appropriate.<br>Breaking the intervals in half to meet the criterion.</p><h3 id="Definition-3"><a href="#Definition-3" class="headerlink" title="Definition"></a>Definition</h3><p>By Trapezoid Rule: $\int^{b}_{a}f(x)dx=S_{[a,b]}-h^{3}\frac{f’’(c_{0})}{12}$<br>$\Rightarrow \int^{b}_{a}f(x)dx=S_{[a,c]}+S_{[c,b]}- \frac{h^{3}}{4}\frac{f’’(c_{3})}{12}$<br>$\Rightarrow S_{[a,b}]-(S_{[a,c]}+S_{[c,b]})\approx \frac{3}{4}h^{3}\frac{f’’(c_{3})}{12}$ is 3-times the size of the error $\frac{h^{3}}{4}\frac{f’’(c_{3})}{12}$<br>So we just need to check $S_{[a,b]}-(S_{[a,c]}+S_{[c,b]}) &lt; 3\cdot \text{TOL} \dot (\frac{b-a}{b_{orig}-a_{orig}})$<br>else, repeat above recursively for $[a,c]$ and $[c,b]$</p><h2 id="Gaussian-Quadrature"><a href="#Gaussian-Quadrature" class="headerlink" title="Gaussian Quadrature"></a>Gaussian Quadrature</h2><h3 id="Orthogonal"><a href="#Orthogonal" class="headerlink" title="Orthogonal"></a>Orthogonal</h3><h4 id="Definition-4"><a href="#Definition-4" class="headerlink" title="Definition"></a>Definition</h4><p>The set of nonzero functions ${ p_{0},\dots,p_{n}}$ on the interval $[a,b]$ is orthogonal on $[a,b]$ if</p><script type="math/tex; mode=display">\int^{b}_{a}p_{j}(x)p_{k}(x)dx=0 \ \ j\ne k</script><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><ol><li>If ${ p_{0},\dots,p_{n}}$ is an orthogonal set of polynomials on the interval $[a,b]$, where $deg \ p_{i}=i$ , then ${ p_{0},\dots,p_{n}}$ is a basis for the vector space of degree at most $n$ polynomials on $[a,b]$. </li><li>If ${ p_{0},\dots,p_{n}}$ is an orthogonal set of polynomials on $[a,b]$ and if $deg \ p_{i}=i$, then $p_{i}$ has $i$ distinct roots in the interval $(a,b)$.</li></ol><h4 id="Legendre-Polynomials"><a href="#Legendre-Polynomials" class="headerlink" title="Legendre Polynomials"></a>Legendre Polynomials</h4><script type="math/tex; mode=display">p_{i}(x)= \frac{1}{2^{i}i!} \frac{d^{i}}{dx^{i}}[(x^{2}-1)^{i}]</script><p>For $0\le i\le n$ is orthogonal on $[-1,1]$.</p><h3 id="Gaussian-Quadrature-1"><a href="#Gaussian-Quadrature-1" class="headerlink" title="Gaussian Quadrature"></a>Gaussian Quadrature</h3><p>Gaussian Quadrature of a function is simply a linear combination of function evaluations at the Legendre roots.</p><h4 id="Definition-5"><a href="#Definition-5" class="headerlink" title="Definition"></a>Definition</h4><p>Fix an $n$, Let $Q(x)$ be the interpolating polynomial for the integrand $f(x)$ at the nodes $x_{1},\dots,x_{n}$, using Lagrange formulation:</p><script type="math/tex; mode=display">Q(x)=\sum\limits_{i=1}^{n}L_{i}(x)f(x_{i}), \ \text{where} \ L_{i}(x)=\frac{(x-x_{1}) \cdots \overline{(x-x_{i})}\cdots (x-x_{n})}{(x_{i}-x_{1}) \cdots \overline{(x_{i}-x_{i})}\cdots (x_{i}-x_{n})}</script><p>Integrating both sides:</p><script type="math/tex; mode=display">\int^{1}_{-1}f(x)dx\approx\sum\limits_{i=1}^{n}c_{i}f(x_{i})</script><p>where $c_{i}=\int^{1}_{-1}L_{i}(x)dx$</p><h4 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem"></a>Theorem</h4><p>The Gaussian Quadrature Method, using the degree $n$ Legendre polynomial on $[-1,1]$, has degree of precision $2_{n}-1$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Numerical-Differentiation&quot;&gt;&lt;a href=&quot;#Numerical-Differentiation&quot; class=&quot;headerlink&quot; title=&quot;Numerical Differentiation&quot;&gt;&lt;/a&gt;Numerical D</summary>
      
    
    
    
    
    <category term="Numerical" scheme="http://amadeus-z.github.io/tags/Numerical/"/>
    
  </entry>
  
  <entry>
    <title>Chapter_3</title>
    <link href="http://amadeus-z.github.io/2022/12/21/Chapter-3/"/>
    <id>http://amadeus-z.github.io/2022/12/21/Chapter-3/</id>
    <published>2022-12-21T13:20:42.000Z</published>
    <updated>2022-12-28T07:54:20.843Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Interpolating-Functions"><a href="#Interpolating-Functions" class="headerlink" title="Interpolating Functions"></a>Interpolating Functions</h2><h3 id="Lagrange-Interpolation"><a href="#Lagrange-Interpolation" class="headerlink" title="Lagrange Interpolation"></a>Lagrange Interpolation</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p><strong>Example</strong>:Gien three points $(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3})$<br>Then the Lagrange interpolating polynomial $P(x)=y_{1}\frac{(x-x_{2})(x-x_{3})}{(x_{1}-x_{2})(x_{1}-x_{3})}+y_{2}\frac{(x-x_{1})(x-x_{3})}{(x_{2}-x_{1})(x_{2}-x_{3})}+y_{3}\frac{(x-x_{1})(x-x_{2})}{(x_{3}-x_{1})(x_{3}-x_{2})}$</p><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><p>Let $(x_{1},y_{1}),\dots,(x_{n},y_{x})$ be $n$ points in the plane with distinct $x_{i}$. Then there exists one and only one polynomial $P$ of degree $n-1$ or less that satisfies $P(x_{i})=y_{i}$ for $i=1,\dots,n$</p><h3 id="Newton’s-Divided-Differences"><a href="#Newton’s-Divided-Differences" class="headerlink" title="Newton’s Divided Differences"></a>Newton’s Divided Differences</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p>Given $x=[x_{1},\dots,x_{n}],y=[y_{1},\dots,y_{n}]$<br>For $j=1,\dots,n$<br>    $f[x_{j}]=y_{j}$<br>End</p><p>For $i=2,\dots,n$<br>    For $j=1,\dots,n+1-i$<br>        $f[x_{j} \ \dots \ x_{j+i-1} ]=\frac{f[x_{j+1} \ \dots \ x_{j+i-1} ]-f[x_{j} \ \dots \ x_{j+i-2} ]}{(x_{j+i-1}-x_{j})}$<br>    End<br>End</p><p>Then the Interpolating polynomial is $P(x)=\sum\limits_{i=1}^{n}f<a href="x-x_{1}">x_{1} \ \dots \ x_{i}</a>\cdots(x-x_{i-1})$ </p><h3 id="Interpolation-Error-Formula"><a href="#Interpolation-Error-Formula" class="headerlink" title="Interpolation Error Formula"></a>Interpolation Error Formula</h3><h4 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem"></a>Theorem</h4><p>Assume $P(x)$ is the (degree $n-1$ or less) interpolating polynomial fitting the $n$ points $(x_{1},y_{1}),\dots,(x_{n},y_{n})$. The interpolation error is </p><script type="math/tex; mode=display">f(x)-P(x)=\frac{(x-x_{1})(x-x_{2})\cdots(x-x_{n})}{n!}f^{(n)}(c)</script><p>where $c$ lies between the samllest and largest of the numbers $x,x_{1},\dots,x_{n}$</p><h3 id="Runge-Phenomenon"><a href="#Runge-Phenomenon" class="headerlink" title="Runge Phenomenon"></a>Runge Phenomenon</h3><p>The fuction has the same general shape as the triangular bump but the interpolation polynomial wiggle near the ends of the interpolation interval.</p><h2 id="Chebyshev’s-Theorem"><a href="#Chebyshev’s-Theorem" class="headerlink" title="Chebyshev’s Theorem"></a>Chebyshev’s Theorem</h2><p>A particular optimal way of spaceing the base points</p><h3 id="Chebyshev’s-Theorem-1"><a href="#Chebyshev’s-Theorem-1" class="headerlink" title="Chebyshev’s Theorem"></a>Chebyshev’s Theorem</h3><h4 id="Theorem-2"><a href="#Theorem-2" class="headerlink" title="Theorem"></a>Theorem</h4><p>The choice of real numbers $-1\le x_{1},\dots ,x_{n}\le 1$ that makes the value of </p><script type="math/tex; mode=display">\max\limits_{-1\le x\le 1}|(x-x_{1})\cdots(x-x_{n})|</script><p>as small as possible is</p><script type="math/tex; mode=display">x_{i}=\cos{\frac{(2i-1)\pi}{2n}} \ \ \ \text{For} \ i=1,\dots,n</script><p>And the minimum value is $\frac{1}{2^{n-1}}$. In fact the minimum is achieved by</p><script type="math/tex; mode=display">(x-x_{1})\cdots(x-x_{n})=\frac{1}{2^{n-1}}T_{n}(x)</script><p>where $T_{n}(x)$ denotes the degree $n$ Chebyshev polynomial.</p><h4 id="Chebyshev-Polynomials"><a href="#Chebyshev-Polynomials" class="headerlink" title="Chebyshev Polynomials"></a>Chebyshev Polynomials</h4><p>The $n$th Chebyshev Polynomial: $T_{n}(x)=\cos{(n\arccos{x})}$<br>Set $y$ = $\arccos{x}$ $\Rightarrow T_{n+1}(x)+T_{n-1}(x)=2\cos{ny}\cos{y}=2xT_{n}(x)$<br>$\Rightarrow$ the recursion relation: $T_{n+1}(x)=2xT_{n}(x)-T_{n-1}(x)$</p><h4 id="Change-of-Intercal"><a href="#Change-of-Intercal" class="headerlink" title="Change of Intercal"></a>Change of Intercal</h4><p>On the intercal $[a,b]$,</p><script type="math/tex; mode=display">x_{i}=\frac{b+a}{2}+\frac{b-a}{2}\cos{\frac{(2i-1)\pi}{2n}}</script><p>For $i=1,\dots,n$. The inequality</p><script type="math/tex; mode=display">|(x-x_{1})\cdots(x-x_{n})|\le \frac{(\frac{b-a}{2}^{n})}{2^{n-1}}</script><p>holds on $[a,b]$</p><h2 id="Cubic-Splines"><a href="#Cubic-Splines" class="headerlink" title="Cubic Splines"></a>Cubic Splines</h2><p>The idea of splines: using several formulas, each a low-degree polynomial to pass through the data points</p><h3 id="Cubic-spline"><a href="#Cubic-spline" class="headerlink" title="Cubic spline"></a>Cubic spline</h3><h4 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h4><p>A cubic spline $S(x)$ through the data points $(x_{1},y_{1}),\dots,(x_{n},y_{n})$ is a set of cubic polynomials:</p><script type="math/tex; mode=display">S_{n-1}(x)=y_{n-1}+b_{n-1}(x-x_{n-1})+c_{n-1}(x-x_{n-1})^{2}+d_{n-1}(x-x_{n-1})^{3} \ \text{on} \ [x_{n-1},x_{n}]</script><h4 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h4><ol><li>$S_{i}(x_{i})=y_{i}$ and $S_{i}(x_{i+1})=y_{i+1}$ for $i=1,\dots,n-1$</li><li>$S_{i-1}’(x_{i})=S_{i}’(x_{i})$ for $i=2,\dots,n-1$</li><li>$S_{i-1}’’(x_{i})=S_{i}’’(x_{i})$ for $i=2,\dots,n-1$</li></ol><p>Above all there are $3(n-1)$ unknowns with $n-1+2(n-2)=3n-5$ linear equations, so there are infinitely many cubic splines. Then we add two more constraints.</p><ol><li>$S_{1}’’(x_{1})=0$ and $S_{n-1}’’(x_{n})=0$  (Natural spline)</li></ol><p>Then we can solve $3(n-1)$ unknowns with $3(n-1)$ linear equations</p><h2 id="Bezier-Curves"><a href="#Bezier-Curves" class="headerlink" title="Bezier Curves"></a>Bezier Curves</h2><p>$to \ be \ continued$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Interpolating-Functions&quot;&gt;&lt;a href=&quot;#Interpolating-Functions&quot; class=&quot;headerlink&quot; title=&quot;Interpolating Functions&quot;&gt;&lt;/a&gt;Interpolating Fun</summary>
      
    
    
    
    
    <category term="Numerical" scheme="http://amadeus-z.github.io/tags/Numerical/"/>
    
  </entry>
  
  <entry>
    <title>Chapter_2</title>
    <link href="http://amadeus-z.github.io/2022/12/19/Chapter-2/"/>
    <id>http://amadeus-z.github.io/2022/12/19/Chapter-2/</id>
    <published>2022-12-19T13:07:36.000Z</published>
    <updated>2022-12-28T07:54:28.341Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Direct-Method"><a href="#Direct-Method" class="headerlink" title="Direct Method"></a>Direct Method</h2><h3 id="Gaussian-elimination"><a href="#Gaussian-elimination" class="headerlink" title="Gaussian elimination"></a>Gaussian elimination</h3><p>After the elimination is completed, the tableau is upper triangular:</p><script type="math/tex; mode=display">\begin{pmatrix} a_{11} & a_{12} & ... & a_{1n} & | & b_{1} \\ 0 & a_{22} & ... & a_{2n} & | & b_{2} \\ \vdots & \vdots & \ddots & \vdots & | & \vdots \\ 0 & 0 & ... & a_{nn} & | & b_{n} \end{pmatrix}</script><h3 id="LU-Factorization"><a href="#LU-Factorization" class="headerlink" title="LU Factorization"></a>LU Factorization</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>An $m\times n$ matrix $L$ is Lower-triangular if its entries satisfy $l_{ij}=0$ for $i<j$. An $m\times n$ matrix $U$ is Upper-triangular if its entries satisfy $u_{ij}=0$ for $i>j$ </p><p>Let $L_{ij}(-c)$ denote the matrix:</p><script type="math/tex; mode=display">\begin{pmatrix} 1 & 0&0 \\ -c & 1&0 \\ 0&0 &1 \\ \end{pmatrix}</script><p>Applay to $\forall$ matrix A:</p><script type="math/tex; mode=display">\begin{pmatrix} 1 & 0&0 \\ -c & 1&0 \\ 0&0 &1 \\ \end{pmatrix}\begin{pmatrix} a_{11} & a_{12}&a_{13} \\ a_{21} & a_{22}&a_{23} \\ a_{31}&a_{32} &a_{33} \\ \end{pmatrix}=\begin{pmatrix} a_{11} & a_{12}&a_{13} \\ a_{21}-ca_{11} & a_{22}-ca_{12}&a_{23}-ca_{13} \\ a_{31}&a_{32} &a_{33} \end{pmatrix}</script><p>Since $L_{ij}(-c)^{-1}=L_{ij}(c)\Rightarrow L_{ij}(-c) A=U_{A}\Rightarrow A=L_{ij}(c)U_{A}$</p><p>since $\begin{pmatrix} 1 &amp; 0&amp;0 \\ c_{1} &amp; 1&amp;0 \\ 0&amp;0 &amp;1 \\ \end{pmatrix}\begin{pmatrix} 1 &amp; 0&amp;0 \\ 0 &amp; 1&amp;0 \\ c_{2}&amp;0 &amp;1 \\ \end{pmatrix}\begin{pmatrix} 1 &amp; 0&amp;0 \\ 0 &amp; 1&amp;0 \\ 0&amp;c_{3} &amp;1 \\ \end{pmatrix}=\begin{pmatrix} 1 &amp; 0&amp;0 \\ c_{1} &amp; 1&amp;0 \\ c_{2}&amp;c_{3} &amp;1 \\ \end{pmatrix}$</p><p>$\Rightarrow \forall A=LU$<br>$\Rightarrow Ax=B\Leftrightarrow L(Ux)=B$, First solve $Lu=b$ for $u$, then solve $Ux=u$ for $x$.</p><h4 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h4><p>$to \ be \ continued$ </p><h3 id="PA-LU-Factorization"><a href="#PA-LU-Factorization" class="headerlink" title="PA=LU Factorization"></a>PA=LU Factorization</h3><h4 id="Partial-pivoting-protocol"><a href="#Partial-pivoting-protocol" class="headerlink" title="Partial pivoting protocol"></a>Partial pivoting protocol</h4><p>Choose the biggest entry to eliminate the column<br>$\Leftrightarrow$ if $|a_{pj}|\ge |a_{ij}|$ for all $i$ , Exchange the $p$th row to the first row and eliminate the $j$th column.</p><h4 id="Permutation-matrix"><a href="#Permutation-matrix" class="headerlink" title="Permutation matrix"></a>Permutation matrix</h4><p>A permutation matrix is an $n\times n$ matrix consisting of all zeros, except for a single 1 in every row and column. Such as $\begin{pmatrix} 1 &amp; 0&amp;0 \\ 0 &amp; 0&amp;1 \\ 0&amp;1 &amp;0 \\ \end{pmatrix}$.</p><h4 id="PA-LU"><a href="#PA-LU" class="headerlink" title="PA=LU"></a>PA=LU</h4><p>Let permutation matrix $P$ with $A$ to establish a matrix after partial pivoting protocol.<br>$PAx=Pb\Rightarrow LUx=Pb\Rightarrow$ </p><ol><li>$Lc=Pb$ for $c$. </li><li>$Ux=c$ for $x$.</li></ol><h4 id="Euler-Bernoulli-Beam"><a href="#Euler-Bernoulli-Beam" class="headerlink" title="Euler-Bernoulli Beam"></a>Euler-Bernoulli Beam</h4><p>$to \ be \ continued$</p><h2 id="Iterative-Methods"><a href="#Iterative-Methods" class="headerlink" title="Iterative Methods"></a>Iterative Methods</h2><h3 id="Jacobi-Method"><a href="#Jacobi-Method" class="headerlink" title="Jacobi Method"></a>Jacobi Method</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p>A form of FPI for a system of equations.<br>Let $D$ denote the main diagonal of $A$. Then $A=L+D+U\Rightarrow (L+D+U)x=b\Rightarrow x=D^{-1}(b-(L+U)x)$ .<br>$x_{0}=\text{initial vector}$<br>$x_{k+1}=D^{-1}(b-(L+U)x_{k})$</p><h4 id="Strictly-diagonally-dominant"><a href="#Strictly-diagonally-dominant" class="headerlink" title="Strictly diagonally dominant"></a>Strictly diagonally dominant</h4><p>The $n\times n$ matrix $A=(a_{ij})$ is strictly diagonally dominant if $\forall 1\le i\le n,|a_{ii}|\ge \sum\limits_{j\ne i}|a_{ij}|$ </p><h4 id="Convergence"><a href="#Convergence" class="headerlink" title="Convergence"></a>Convergence</h4><p>If the $n\times n$ matrix $A$ is strictly diagonally dominant, then</p><ol><li>$A$ is a nonsingular matrix</li><li>The Jacobi Method applied to $Ax=b$ converges to the unique solution</li></ol><h3 id="Gauss-Seidel-Method"><a href="#Gauss-Seidel-Method" class="headerlink" title="Gauss-Seidel Method"></a>Gauss-Seidel Method</h3><p>Difference from Jacobi method: the most recently updated values of the unknowns are used at each step.</p><h4 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h4><p>$x_{0}=\text{initial vector}$<br>$(L+D)x_{k+1}=-Ux_{k}+b$<br>$x_{k+1}=D^{-1}(b-Ux_{k}-Lx_{k+1})$</p><h4 id="Convergence-1"><a href="#Convergence-1" class="headerlink" title="Convergence"></a>Convergence</h4><p>If the $n\times n$ matrix $A$ is strictly diagonally dominant, then</p><ol><li>$A$ is a nonsingular matrix</li><li>The Gauss-Seidel applied to $Ax=b$ converges to a solution</li></ol><h4 id="Successive-Over-Relaxation-SOR"><a href="#Successive-Over-Relaxation-SOR" class="headerlink" title="Successive Over-Relaxation (SOR)"></a>Successive Over-Relaxation (SOR)</h4><p>A method to speed the convergence of Gauss-Seidel method.<br>$to \ be \ continued$</p><h3 id="Convergence-2"><a href="#Convergence-2" class="headerlink" title="Convergence"></a>Convergence</h3><h4 id="sparse-matrix-computations"><a href="#sparse-matrix-computations" class="headerlink" title="sparse matrix computations"></a>sparse matrix computations</h4><p>$to \ be \ continued$</p><h2 id="Methods-for-symmetric-positive-definite-matrices"><a href="#Methods-for-symmetric-positive-definite-matrices" class="headerlink" title="Methods for symmetric positive-definite matrices"></a>Methods for symmetric positive-definite matrices</h2><h3 id="Symmetric-positive-definite-matrices"><a href="#Symmetric-positive-definite-matrices" class="headerlink" title="Symmetric positive-definite matrices"></a>Symmetric positive-definite matrices</h3><h4 id="Definition-3"><a href="#Definition-3" class="headerlink" title="Definition"></a>Definition</h4><p>The $n\times n$ matrix $A$ is <strong>symmetric</strong> if $A^{T}=A$.<br>The matrix $A$ is <strong>positive-definite</strong> if $x^{T}Ax&gt;0$ for all vectors $x\ne 0$</p><h4 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h4><ol><li>If the $n\times n$ matrix $A$ is symmetric, then $A$ is positive-definite $\Leftrightarrow$ all of its <strong>eigenvalues</strong> are positive</li><li>If $A$ is $n\times n$ symmetric positive-definite and $X$ is an $n\times m$ matrix of full rank with $n\ge m$, then $X^{T}AX$ is $m\times m$ symmetric positive-definite.</li><li>Any <strong>principal submatrix</strong> of a symmetric positive-definite matrix is symmetric positive-definit</li></ol><h3 id="Cholesky-factorization"><a href="#Cholesky-factorization" class="headerlink" title="Cholesky factorization"></a>Cholesky factorization</h3><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><p>If $A$ is a symmetric positive-definite $n\times n$ matrix, then there exists an upper triangular $n\times n$ matrix $R$ such that $A=R^{T}R$</p><p><strong>Example</strong>:<br>$A=\begin{pmatrix}a  &amp; b \\ b &amp; c \end{pmatrix}=\begin{pmatrix} \sqrt{a}  &amp; 0 \\ \frac{b}{\sqrt{a}} &amp; \sqrt{c-\frac{b^{2}}{a}} \end{pmatrix}\begin{pmatrix} \sqrt{a}  &amp; \frac{b}{\sqrt{a}} \\ 0 &amp; \sqrt{c-\frac{b^{2}}{a}} \end{pmatrix}=R^{T}R$</p><h3 id="Conjugate-Gradient-Method"><a href="#Conjugate-Gradient-Method" class="headerlink" title="Conjugate Gradient Method"></a>Conjugate Gradient Method</h3><p>$to \ be \ continued$</p><h2 id="Nonlinear-Systems-of-Equations"><a href="#Nonlinear-Systems-of-Equations" class="headerlink" title="Nonlinear Systems of Equations"></a>Nonlinear Systems of Equations</h2><h3 id="Multivariate-Newton’s-Method"><a href="#Multivariate-Newton’s-Method" class="headerlink" title="Multivariate Newton’s Method"></a>Multivariate Newton’s Method</h3><h4 id="Definition-4"><a href="#Definition-4" class="headerlink" title="Definition"></a>Definition</h4><p>Denote a system of equation ($f_{i}(u,v,w)=0$) by $F(u,v,w)=(f_{i},…)$.<br>The Taylor expansion for vector-valued functions around $x_{0}$ is:<br>$F(x)=F(x_{0})+DF(x_{0})(x-x_{0})+O(x-x_{0})^{2}$<br>$\Rightarrow 0=F(r)\approx F(x_{0})+DF(x_{0})(r-x_{0})$<br>$\Rightarrow \ \ x_{0}=\text{initial vector}$<br>$\Rightarrow \ \ x_{k+1}=x_{k}-(DF(x_{k}))^{-1}F(x_{k})$</p><h3 id="Broyden’s-Method"><a href="#Broyden’s-Method" class="headerlink" title="Broyden’s Method"></a>Broyden’s Method</h3><p>$to \ be \ continued$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Direct-Method&quot;&gt;&lt;a href=&quot;#Direct-Method&quot; class=&quot;headerlink&quot; title=&quot;Direct Method&quot;&gt;&lt;/a&gt;Direct Method&lt;/h2&gt;&lt;h3 id=&quot;Gaussian-elimination&quot;</summary>
      
    
    
    
    
    <category term="Numerical" scheme="http://amadeus-z.github.io/tags/Numerical/"/>
    
  </entry>
  
  <entry>
    <title>Chapter_1</title>
    <link href="http://amadeus-z.github.io/2022/12/16/Chapter-1/"/>
    <id>http://amadeus-z.github.io/2022/12/16/Chapter-1/</id>
    <published>2022-12-16T13:45:45.000Z</published>
    <updated>2022-12-16T13:47:30.193Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Bisection-Method"><a href="#Bisection-Method" class="headerlink" title="Bisection Method"></a>Bisection Method</h3><p><strong>Code</strong>:<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">xc</span>=<span class="title">bisect</span><span class="params">(f,a,b,tol)</span></span></span><br><span class="line">fa=f(a)</span><br><span class="line">fb=f(b)</span><br><span class="line"><span class="keyword">while</span> (b-a)/<span class="number">2</span>&gt;tol</span><br><span class="line">c=(a+b)/<span class="number">2</span>;</span><br><span class="line">fc=f(c);</span><br><span class="line"><span class="keyword">if</span> f(c)==<span class="number">0</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">sign</span>(fc)*<span class="built_in">sign</span>(fa)&lt;<span class="number">0</span></span><br><span class="line">b=c;</span><br><span class="line">fb=f(c);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">a=c;</span><br><span class="line">fa=f(c);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">xc=(a+b)/<span class="number">2</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>Solution error $=|x_{c}-r|&lt;\frac{b-a}{2^{n+1}}$ </p><h3 id="Fixed-Point-Iteration"><a href="#Fixed-Point-Iteration" class="headerlink" title="Fixed-Point Iteration"></a>Fixed-Point Iteration</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>fixed point $r$ of function $g$ : $g(r)=r\Leftrightarrow g(r)-r=0$ </p><p>Let $x_{i+1}=g(x_{i})$, if $g$ continuous and $x_{i}\rightarrow r$ , Then:</p><script type="math/tex; mode=display">g(r)=g(\lim x_{i})=\lim g(x_{i})=\lim x_{i+1}=r</script><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">xc</span>=<span class="title">fpi</span><span class="params">(g,x0,k)</span></span></span><br><span class="line">x(<span class="number">1</span>)=x0;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:k</span><br><span class="line">x(<span class="built_in">i</span>+<span class="number">1</span>)=g(x(<span class="built_in">i</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">xc=x(k+<span class="number">1</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Linear-Convergence"><a href="#Linear-Convergence" class="headerlink" title="Linear Convergence"></a>Linear Convergence</h4><p>Let $e_{i}=|r-x_{i}|$ denote the error at the step $i$ of an iterative method. If</p><script type="math/tex; mode=display">\lim\limits_{i\rightarrow\infty}\frac{e_{i+1}}{e_{i}}=S<1</script><p>Then the method is said to obey Linear Convergence with rate $S$.</p><h4 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h4><p>$g$ is continuously differentiable with $g(r)=r$ such that $S=|g’(r)|&lt;1$ .<br>Then FPI(Fixed-Point Iteration) converges linearly with rate $S$ to the fixed point $r$ for initial guesses sufficiently close to $r$.</p><p><strong>Proof:</strong><br>$x_{i+1}-r=g(x_{i})-g(r)=g’(c_{i})(x_{i}-r)$<br>$\Rightarrow e_{i+1}=|g’(c_{i})|e_{i}$</p><h4 id="Locally-Convergent"><a href="#Locally-Convergent" class="headerlink" title="Locally Convergent"></a>Locally Convergent</h4><p>An iterative method is called <strong>locally convergent</strong> to $r$ if the method converges to $r$ for initial guesses sufficiently close to $r$.</p><h4 id="Stopping-Criteria"><a href="#Stopping-Criteria" class="headerlink" title="Stopping Criteria"></a>Stopping Criteria</h4><p>For a set tolerance <strong>TOL</strong>, define  some error stopping criterion:</p><script type="math/tex; mode=display">f(x_{i},x_{i+1})<TOL</script><h3 id="Sensitivity"><a href="#Sensitivity" class="headerlink" title="Sensitivity"></a>Sensitivity</h3><h4 id="Define"><a href="#Define" class="headerlink" title="Define"></a>Define</h4><p>Assume we find a root $r$ of $f(x)=0$, with a small change $\epsilon g(x)$ is made to the input. Let $\Delta r$ be the corresponding change in the root such that</p><script type="math/tex; mode=display">f(r+\Delta r)+\epsilon g(r+\Delta r)=0</script><p>Expanding in Talyor polynomials $\Rightarrow$</p><script type="math/tex; mode=display">\Delta r\approx-\frac{\epsilon g(r)}{f'(r)}</script><p><strong>Error magnification factor</strong>:</p><script type="math/tex; mode=display">\frac{\text{relative forward error}}{\text{relative backward error}}=\frac{\Delta r/r}{\epsilon g(r)/g(r)}=|\frac{g(r)}{rf'(r)}|</script><h3 id="Newton’s-Method"><a href="#Newton’s-Method" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p>$f’(x_{0})(x-x_{0})=0-f(x_{0})\Rightarrow x=x_{0}-\frac{f(x_{0})}{f’(x_{0})}\Rightarrow  \begin{align}  &amp; x_{0}=\text{initial guess} \\ &amp; x_{i+1}=x_{i}-\frac{f(x_{i})}{f’(x_{i})}\end{align}$</p><h4 id="Quadratic-convergence"><a href="#Quadratic-convergence" class="headerlink" title="Quadratic convergence"></a>Quadratic convergence</h4><p>The iteration is quadratically convergent if</p><script type="math/tex; mode=display">M=\lim\limits_{i\rightarrow\infty} \frac{e_{i+1}}{e_{i}^{2}}<\infty</script><p>By Taylor’s theorem: $f(r)=f(x_{i})+(r-x_{i})f’(x_{i})+\frac{(r-x_{i})^{2}}{2}f’’(c_{i})$<br>$\Rightarrow x_{i+1}-r=\frac{(r-x_{i})^{2}}{2}\frac{f’’(c_{i})}{f’(x_{i})}\Rightarrow \frac{e_{i+1}}{e_{i}^{2}}=|\frac{f’’(c_{i})}{2f’(x_{i})}|\Rightarrow \lim\limits_{i\rightarrow\infty} \frac{e_{i+1}}{e_{i}^{2}}=|\frac{f’’(r)}{2f’(r)}|$ </p><h4 id="Modified-Newton’s-Method"><a href="#Modified-Newton’s-Method" class="headerlink" title="Modified Newton’s Method"></a>Modified Newton’s Method</h4><p>If $f$ is $(m+1)$-times continuously differentiable on $[a,b]$, which contains a root $r$ of multiplicity $m&gt;1$, then <strong>Modified Newton’s Method</strong></p><script type="math/tex; mode=display">x_{i+1}=x_{i}-\frac{mf(x_{i})}{f'(x_{i})}</script><p>converges locally and quadratically to $r$.</p><h3 id="Secant-Method"><a href="#Secant-Method" class="headerlink" title="Secant Method"></a>Secant Method</h3><h4 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h4><p>Replace $f’(x_{i})$ in Newton’s Method by $\frac{f(x_{i})-f(x_{i-1})}{x_{i}-x_{i-1}}$<br>$x_{0},x_{1}=\text{initial guesses}$<br>$x_{i+1}=x_{i}-\frac{f(x_{i})(x_{i}-x_{i-1})}{f(x_{i})-f(x_{i-1})}$<br>$e_{i+1}\approx |\frac{f’’(r)}{2f’(r)}|e_{i}e_{i-1}\approx |\frac{f’’(r)}{2f’(r)}|^{\alpha-1}e_{i}^{\alpha}$                      $\alpha=\frac{1+\sqrt{5}}{2}$   </p><h4 id="Method-of-False-Position-Regula-Falsi"><a href="#Method-of-False-Position-Regula-Falsi" class="headerlink" title="Method of False Position / Regula Falsi"></a>Method of False Position / Regula Falsi</h4><p>Replace the midpoint in Bisection Method by Secant Method:<br>$c=a-\frac{f(a)(a-b)}{f(a)-f(b)}=\frac{bf(a)-af(b)}{f(a)-f(b)}$ </p><h4 id="Muller’s-Method"><a href="#Muller’s-Method" class="headerlink" title="Muller’s Method"></a>Muller’s Method</h4><p><a href="https://en.wikipedia.org/wiki/Muller%27s_method">wiki</a></p><h4 id="Inverse-Quadratic-Interpolation-IQI"><a href="#Inverse-Quadratic-Interpolation-IQI" class="headerlink" title="Inverse Quadratic Interpolation (IQI)"></a>Inverse Quadratic Interpolation (IQI)</h4><p><a href="https://en.wikipedia.org/wiki/Inverse_quadratic_interpolation">wiki</a></p><h4 id="Brent’s-Method"><a href="#Brent’s-Method" class="headerlink" title="Brent’s Method"></a>Brent’s Method</h4><p><a href="https://en.wikipedia.org/wiki/Brent%27s_method">wki</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Bisection-Method&quot;&gt;&lt;a href=&quot;#Bisection-Method&quot; class=&quot;headerlink&quot; title=&quot;Bisection Method&quot;&gt;&lt;/a&gt;Bisection Method&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Code&lt;/</summary>
      
    
    
    
    
    <category term="Numerical" scheme="http://amadeus-z.github.io/tags/Numerical/"/>
    
  </entry>
  
  <entry>
    <title>Chapter_0</title>
    <link href="http://amadeus-z.github.io/2022/12/13/Chapter-0/"/>
    <id>http://amadeus-z.github.io/2022/12/13/Chapter-0/</id>
    <published>2022-12-13T11:54:28.000Z</published>
    <updated>2022-12-13T12:16:48.815Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Nested-multiplication-Horner’s-method"><a href="#Nested-multiplication-Horner’s-method" class="headerlink" title="Nested multiplication / Horner’s method"></a>Nested multiplication / Horner’s method</h3><p><strong>Example:</strong><br>$\begin{align}  P(x) &amp;=2x^{4}+3x^{3}-3x^{2}+5x-1\\ &amp;=-1+x(5+x(-3+x(3+x\cdot 2))) \end{align}$</p><p><strong>Code:</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% d: degree of polynomial</span></span><br><span class="line"><span class="comment">% c: coefficients array</span></span><br><span class="line"><span class="comment">% b: array of base points b</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span>=<span class="title">nest</span><span class="params">(d,c,x,b)</span></span></span><br><span class="line"><span class="keyword">if</span> nargin&lt;<span class="number">4</span></span><br><span class="line">b=<span class="built_in">zeros</span>(d,<span class="number">1</span>); </span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">y = c(d+<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=d:<span class="number">-1</span>:<span class="number">1</span></span><br><span class="line">y = =y.*(x-b(<span class="built_in">i</span>))+c(<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="Decimal-to-Binary"><a href="#Decimal-to-Binary" class="headerlink" title="Decimal to Binary"></a>Decimal to Binary</h3><p><strong>Example</strong>: $(0.7)_{10}\rightarrow (.1\overline{0110})_{2}$</p><p>$\begin{align} .7\times 2=.4+1 \\ .4\times2=.8+0 \\ .8\times 2=.6+1 \\ .6\times 2=.2+1 \\ .2\times 2=.4+0 \\ .4\times 2=.8+0 \end{align}$</p><h3 id="Binary-to-Decimal"><a href="#Binary-to-Decimal" class="headerlink" title="Binary to Decimal"></a>Binary to Decimal</h3><p><strong>Example 1</strong>: $(.1011)_{2}\rightarrow (\frac{11}{16})_{10}$ </p><p>$(.1011)_{2}=1\cdot \frac{1}{2}^{1}+0\cdot \frac{1}{2}^{2}+1\cdot \frac{1}{2}^{3}+1\cdot \frac{1}{2}^{4}=(\frac{11}{16})_{10}$  </p><p><strong>Example 2</strong>: $(.\overline{1011})_{2}$</p><p>Let $x=(.\overline{1011})_{2}\Rightarrow 2^{4}x=(1011.\overline{1011})_{2}\Rightarrow (2^{4}-1)x=(1011)_{2}=(11)_{10}$</p><h3 id="Floating-Point-Representation-of-Real-Numbers"><a href="#Floating-Point-Representation-of-Real-Numbers" class="headerlink" title="Floating Point Representation of Real Numbers"></a>Floating Point Representation of Real Numbers</h3><div class="table-container"><table><thead><tr><th>Precision</th><th>sign</th><th>exponent</th><th>mantissa</th></tr></thead><tbody><tr><td>single</td><td>1</td><td>8</td><td>23</td></tr><tr><td>double</td><td>1</td><td>11</td><td>52</td></tr><tr><td>long double</td><td>1</td><td>15</td><td>64</td></tr></tbody></table></div><h3 id="The-Intermediate-Value-Theorem"><a href="#The-Intermediate-Value-Theorem" class="headerlink" title="The Intermediate Value Theorem"></a>The Intermediate Value Theorem</h3><p><a href="https://en.wikipedia.org/wiki/Intermediate_value_theorem">wiki</a></p><h3 id="The-Mean-Value-Theorem"><a href="#The-Mean-Value-Theorem" class="headerlink" title="The Mean Value Theorem"></a>The Mean Value Theorem</h3><p><a href="https://en.wikipedia.org/wiki/Mean_value_theorem">wiki</a></p><h3 id="Taylor’s-Theorem"><a href="#Taylor’s-Theorem" class="headerlink" title="Taylor’s Theorem"></a>Taylor’s Theorem</h3><p><a href="https://en.wikipedia.org/wiki/Taylor%27s_theorem">wiki</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Nested-multiplication-Horner’s-method&quot;&gt;&lt;a href=&quot;#Nested-multiplication-Horner’s-method&quot; class=&quot;headerlink&quot; title=&quot;Nested multiplicat</summary>
      
    
    
    
    
    <category term="Numerical" scheme="http://amadeus-z.github.io/tags/Numerical/"/>
    
  </entry>
  
  <entry>
    <title>Anything</title>
    <link href="http://amadeus-z.github.io/2022/11/02/Anything/"/>
    <id>http://amadeus-z.github.io/2022/11/02/Anything/</id>
    <published>2022-11-02T08:08:13.000Z</published>
    <updated>2022-11-02T08:08:13.641Z</updated>
    
    
    
    
    
  </entry>
  
</feed>
